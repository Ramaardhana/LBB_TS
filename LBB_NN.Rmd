---
title: "LBB Neural Network"
author: "Rama Ardhana"
date: "10/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r warning=FALSE, message=FALSE}
library(dplyr)
library(keras)
library(caret)
```

```{r}
sign_train <- read.csv("data_input/sign_mnist_train.csv")
sign_test <- read.csv ("data_input/sign_mnist_test.csv")
```


#EDA
```{r}
sign_train %>% head(3)
```

```{r}
colSums(is.na(sign_train))
```


```{r}
# your code here
unique(sign_train$label)
```

#Preparasi Data

```{r}
sign_train <- sign_train %>% 
  mutate(label = ifelse(label > 9, label-1, label))

sign_test <- sign_test %>% 
  mutate(label = ifelse(label > 9, label-1, label))
```



```{r}
# Predictor variables in `sign_train`
train_x <- sign_train %>% 
  select(-label) %>% 
  as.matrix() / 255

# Predictor variables in `sign_test`
test_x <- sign_test %>% 
  select(-label) %>% 
  as.matrix() / 255


# Target variable in `sign_train`
train_y <- to_categorical(sign_train$label,num_classes = 24)

# Target variable in `sign_test`
test_y <- to_categorical(sign_test$label,num_classes = 24)
```



```{r}
# Predictor variables in `train_x`
train_x_array <- array_reshape(train_x, dim(train_x))

# Predictor variables in `test_x`
test_x_array <- array_reshape(test_x, dim(test_x))
```

melakukan *one-hot encoding* terhadap target variabel pada data train (`train_y`). 
```{r}
# Target variable in `train_y`
train_y_dummy <- to_categorical(sign_train$label,num_classes = 24)
```


#Membangun Arsitektur
```{r}
RNGkind(sample.kind = "Rounding")
set.seed(100)
initializer <- initializer_random_normal(seed = 100)
```


```{r}
model_base <- keras_model_sequential(name = "MNIST_base") %>% 
  layer_dense(units = 64, # 64 neuron di hidden layer pertama
              activation = "relu", # karena terbaik untuk data gambar
              input_shape = 784, #ada berapa kolom atau input
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "hidden_1") %>% 
  layer_dense(units = 32, #32 neuron di hidden layer ke 2
              activation = "relu",
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "hidden_2") %>%
  layer_dense(units = 24, # 14 neuron di output layer
              activation = "softmax",
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "output")

summary(model_base)
```
```{r}
save_model_tf(model_base, filepath = "model_base")
```
```{r}
model_base <- load_model_tf("model_base")
```

```{r}

model_bigger <- keras_model_sequential(name = "MNIST_bigger") %>% 
  layer_dense(units = 256, # 256 neuron di hidden layer pertama
              activation = "relu", # karena terbaik untuk data gambar
              input_shape = 784, #ada berapa kolom atau input
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "hidden_1") %>% 
  layer_dense(units = 128, #128 neuron di hidden layer ke 2
              activation = "relu",
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "hidden_2") %>%
  layer_dense(units = 64, #64 neuron di hidden layer ke 2
              activation = "relu",
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "hidden_3") %>%
  layer_dense(units = 24, # sesuai jumlah kategori target
              activation = "softmax",
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "output")

model_bigger
```
```{r}
save_model_tf(model_bigger, filepath = "model_bigger")
model_bigger <- load_model_tf("model_bigger")
```

# Compile Model 

```{r}
# your code here
model_base %>% 
  compile(loss = "categorical_crossentropy", #error function 
          metrics = "accuracy", #optional boleh ada atau tidak 
          optimizer = optimizer_sgd(learning_rate = 0.01)
          # menggunakan gradient decent dengan learning rate 0.01
  )
```

```{r}
# your code here
model_bigger %>% 
  compile(loss = "categorical_crossentropy", # error function 
          metrics = "accuracy", #optional boleh ada atau tidak 
          optimizer = optimizer_adam(learning_rate = 0.001)
          # menggunakan gradient decent dengan learning rate 0.001
  )
```

#Fitting model

```{r}

history_base <- model_base %>% 
  fit( x = train_x_array, 
       y = train_y,
       batch_size = 150,
       epoch = 10, #berapa iterasi
       validation_data = list(test_x_array,test_y), # cek data test
       verbose = 1, # 1 artinya mau menampilkan
       shuffle = F) #agar sampel pada tiap batch tidak diambil secara random

plot(history_base)
```

```{r}
# your code here
history_bigger <- model_bigger %>% 
  fit( x = train_x_array,
       y = train_y_dummy,
       batch_size = 150,
       epoch = 10,
       validation_data = list(test_x_array,test_y), 
       shuffle = F)

plot(history_bigger)
```

# Prediksi ke data test

```{r}
# your code here
pred_base <- model_base %>% predict(test_x_array) %>% k_argmax %>% as.array

pred_bigger <-model_bigger %>% predict(test_x_array) %>% k_argmax()%>% as.array
```



#Evaluasi Model Base & Bigger
```{r}

confusionMatrix(as.factor(pred_base), as.factor(sign_test$label))
```
```{r}
confusionMatrix(as.factor(pred_bigger), as.factor(sign_test$label))
```

  
#4.2 Model Tuning


```{r}

model_tuning <- keras_model_sequential(name = "MNIST_tuning") %>% 
  layer_dense(units = 512, #neuron di hidden layer pertama
              activation = "relu", #karena terbaik untuk data gambar
              input_shape = 784, #ada berapa kolom prediktor
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "hidden_1") %>% 
    layer_dense(units = 128, #64 neuron di hidden layer ke 2
              activation = "relu",
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "hidden_2") %>%
  layer_dense(units = 24, # sesuai jumlah kategori target
              activation = "softmax",
              kernel_initializer = initializer,
              bias_initializer = initializer,
              name = "output")

model_tuning
```

```{r}
save_model_tf(model_tuning, filepath = "model_tuning")
model_tuning <- load_model_tf("model_tuning")
```

```{r}
model_tuning %>% 
  compile(loss = "categorical_crossentropy",  
          metrics = "accuracy",  
          optimizer = optimizer_adam(learning_rate = 0.0001)#mengurangi learning rate
          
  )
```



```{r}
# your code here
history_tuning <- model_tuning %>% 
  fit( x = train_x_array,
       y = train_y_dummy,
       batch_size = 150,
       epoch = 20,
       validation_data = list(test_x_array,test_y), 
       shuffle = F)

plot(history_tuning)
```
Model tuning terlihat overfit


#Prediksi Model

```{r}
# your code here
pred_tuning <- model_tuning %>% predict(test_x_array) %>% k_argmax %>% as.array
```

#Evaluasi Model

```{r}
# your code here
confusionMatrix(as.factor(pred_tuning), as.factor(sign_test$label))
```

